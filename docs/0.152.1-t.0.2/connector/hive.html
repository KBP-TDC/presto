

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>12.2. Hive Connector &mdash; Teradata Distribution of Presto 0.152.1-t.0.2 Documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/teradata.css" type="text/css" />
  

  

  
    <link rel="top" title="Teradata Distribution of Presto 0.152.1-t.0.2 Documentation" href="../index.html"/>
        <link rel="up" title="12. Teradata Supported Connectors" href="../connector.html"/>
        <link rel="next" title="12.3. Hive Security Configuration" href="hive-security.html"/>
        <link rel="prev" title="12.1. Teradata Connector" href="teradata.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Teradata Distribution of Presto
          

          
          </a>

          
            
            
              <div class="version">
                0.152.1-t.0.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">1. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system-requirements.html">2. System Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">3. Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sandbox-vms.html">4. Presto Installation on a Sandbox VM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../server-installation-cluster-manual.html">5. Presto Server Installation on a Cluster (Presto Admin and RPMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../server-installation-aws-emr-manual.html">6. Presto Server Installation on an AWS EMR (Presto Admin and RPMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../server-installation-other.html">7. Alternative Installation Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../client.html">8. Presto Client Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/presto-admin/user-guide.html">9. Presto Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security.html">10. Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin.html">11. Administration</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../connector.html">12. Teradata Supported Connectors</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="teradata.html">12.1. Teradata Connector</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">12.2. Hive Connector</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#supported-file-types">Supported File Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuration">Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multiple-hive-clusters">Multiple Hive Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hdfs-configuration">HDFS Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hdfs-username">HDFS Username</a></li>
<li class="toctree-l4"><a class="reference internal" href="#accessing-hadoop-clusters-protected-with-kerberos-authentication">Accessing Hadoop clusters protected with Kerberos authentication</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hdfs-permissions">HDFS Permissions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#configuration-properties">Configuration Properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="#amazon-s3-configuration">Amazon S3 Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sql-limitation-for-s3-tables">SQL Limitation for S3 tables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#amazon-s3-configuration-properties">Amazon S3 Configuration Properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="#querying-hive-tables">Querying Hive Tables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#clustered-hive-tables-support">Clustered hive tables support</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tuning">Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hive-assume-canonical-partition-keys"><code class="docutils literal"><span class="pre">hive.assume-canonical-partition-keys</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-domain-compaction-threshold"><code class="docutils literal"><span class="pre">hive.domain-compaction-threshold</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-force-local-scheduling"><code class="docutils literal"><span class="pre">hive.force-local-scheduling</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-initial-split-size"><code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-initial-splits"><code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-outstanding-splits"><code class="docutils literal"><span class="pre">hive.max-outstanding-splits</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-partitions-per-writers"><code class="docutils literal"><span class="pre">hive.max-partitions-per-writers</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-split-iterator-threads"><code class="docutils literal"><span class="pre">hive.max-split-iterator-threads</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-split-size"><code class="docutils literal"><span class="pre">hive.max-split-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-metastore-partition-batch-size-max"><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-metastore-partition-batch-size-min"><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-max-buffer-size"><code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-max-merge-distance"><code class="docutils literal"><span class="pre">hive.orc.max-merge-distance</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-stream-buffer-size"><code class="docutils literal"><span class="pre">hive.orc.stream-buffer-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-use-column-names"><code class="docutils literal"><span class="pre">hive.orc.use-column-names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-parquet-optimized-reader-enabled"><code class="docutils literal"><span class="pre">hive.parquet-optimized-reader.enabled</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-parquet-predicate-pushdown-enabled"><code class="docutils literal"><span class="pre">hive.parquet-predicate-pushdown.enabled</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-parquet-use-column-names"><code class="docutils literal"><span class="pre">hive.parquet.use-column-names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-s3-max-connections"><code class="docutils literal"><span class="pre">hive.s3.max-connections</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-s3-multipart-min-file-size"><code class="docutils literal"><span class="pre">hive.s3.multipart.min-file-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-s3-multipart-min-part-size"><code class="docutils literal"><span class="pre">hive.s3.multipart.min-part-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#orc-max-buffer-size"><code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#orc-max-merge-distance"><code class="docutils literal"><span class="pre">orc_max_merge_distance</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#orc-stream-buffer-size"><code class="docutils literal"><span class="pre">orc_stream_buffer_size</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#hive-connector-limitations">Hive Connector Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hive-security.html">12.3. Hive Security Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="mysql.html">12.4. MySQL Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="postgresql.html">12.5. PostgreSQL Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="system.html">12.6. System Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="tpch.html">12.7. TPCH Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="blackhole.html">12.8. Black Hole Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="jmx.html">12.9. JMX Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="memory.html">12.10. Memory Connector</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../community_connector.html">13. Community Supported Connectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functions.html">14. Functions and Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../language.html">15. SQL Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sql.html">16. SQL Statement Syntax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">17. Migration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../develop.html">18. Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release.html">19. Release Notes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Teradata Distribution of Presto</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
        <li><a href="../connector.html">12. Teradata Supported Connectors</a> &raquo;</li>
      
    <li>12.2. Hive Connector</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="../_sources/connector/hive.txt" rel="nofollow"> View page source</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hive-connector">
<h1>12.2. Hive Connector</h1>
<p>The Hive connector allows querying data stored in a Hive
data warehouse. Hive is a combination of three components:</p>
<ul class="simple">
<li>Data files in varying formats that are typically stored in the
Hadoop Distributed File System (HDFS) or in Amazon S3.</li>
<li>Metadata about how the data files are mapped to schemas and tables.
This metadata is stored in a database such as MySQL and is accessed
via the Hive metastore service.</li>
<li>A query language called HiveQL. This query language is executed
on a distributed computing framework such as MapReduce or Tez.</li>
</ul>
<p>Presto only uses the first two components: the data and the metadata.
It does not use HiveQL or any part of Hive&#8217;s execution environment.</p>
<div class="section" id="supported-file-types">
<h2>Supported File Types</h2>
<p>The following file types are supported for the Hive connector:</p>
<ul class="simple">
<li>ORC</li>
<li>Parquet</li>
<li>RCFile</li>
<li>SequenceFile</li>
<li>Text</li>
</ul>
</div>
<div class="section" id="configuration">
<h2>Configuration</h2>
<p>Presto includes Hive connectors for multiple versions of Hadoop:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">hive-hadoop2</span></code>: Apache Hadoop 2.x</li>
<li><code class="docutils literal"><span class="pre">hive-cdh5</span></code>: Cloudera CDH 5</li>
</ul>
<p>Create <code class="docutils literal"><span class="pre">/etc/opt/prestoadmin/connector/hive.properties</span></code> with the following contents
to mount the <code class="docutils literal"><span class="pre">hive-hadoop2</span></code> connector as the <code class="docutils literal"><span class="pre">hive</span></code> catalog,
replacing <code class="docutils literal"><span class="pre">hive-hadoop2</span></code> with the proper connector for your version
of Hadoop and <code class="docutils literal"><span class="pre">example.net:9083</span></code> with the correct host and port
for your Hive metastore Thrift service:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>connector.name=hive-hadoop2
hive.metastore.uri=thrift://example.net:9083
</pre></div>
</div>
<p>Use <code class="docutils literal"><span class="pre">presto-admin</span></code> to deploy the connector file. See <a class="reference internal" href="../installation/presto-admin/installation/presto-connector-installation.html"><span class="doc">Adding a Connector</span></a>.</p>
<div class="section" id="multiple-hive-clusters">
<h3>Multiple Hive Clusters</h3>
<p>You can have as many catalogs as you need, so if you have additional
Hive clusters, simply add another properties file to <code class="docutils literal"><span class="pre">/etc/presto/catalog</span></code>
with a different name (making sure it ends in <code class="docutils literal"><span class="pre">.properties</span></code>). For
example, if you name the property file <code class="docutils literal"><span class="pre">sales.properties</span></code>, Presto
will create a catalog named <code class="docutils literal"><span class="pre">sales</span></code> using the configured connector.</p>
</div>
<div class="section" id="hdfs-configuration">
<h3>HDFS Configuration</h3>
<p>For basic setups, Presto configures the HDFS client automatically and
does not require any configuration files. In some cases, such as when using
federated HDFS or NameNode high availability, it is necessary to specify
additional HDFS client options in order to access your HDFS cluster. To do so,
add the <code class="docutils literal"><span class="pre">hive.config.resources</span></code> property to reference your HDFS config files:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
</pre></div>
</div>
<p>Only specify additional configuration files if necessary for your setup.
We also recommend reducing the configuration files to have the minimum
set of required properties, as additional properties may cause problems.</p>
<p>The configuration files must exist on all Presto nodes. If you are
referencing existing Hadoop config files, make sure to copy them to
any Presto nodes that are not running Hadoop.</p>
</div>
<div class="section" id="hdfs-username">
<h3>HDFS Username</h3>
<p>When not using Kerberos with HDFS, Presto will access HDFS using the
OS user of the Presto process. For example, if Presto is running as
<code class="docutils literal"><span class="pre">nobody</span></code>, it will access HDFS as <code class="docutils literal"><span class="pre">nobody</span></code>. You can override this
username by setting the <code class="docutils literal"><span class="pre">HADOOP_USER_NAME</span></code> system property in the
Presto <code class="docutils literal"><span class="pre">presto_jvm_config</span></code>, replacing <code class="docutils literal"><span class="pre">hdfs_user</span></code> with the
appropriate username:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>-DHADOOP_USER_NAME=hdfs_user
</pre></div>
</div>
</div>
<div class="section" id="accessing-hadoop-clusters-protected-with-kerberos-authentication">
<h3>Accessing Hadoop clusters protected with Kerberos authentication</h3>
<p>Kerberos authentication is currently supported for both HDFS and the Hive
metastore.</p>
<p>However there are still a few limitations:</p>
<ul class="simple">
<li>Kerberos authentication is only supported for the <code class="docutils literal"><span class="pre">hive-hadoop2</span></code> and
<code class="docutils literal"><span class="pre">hive-cdh5</span></code> connectors.</li>
<li>Kerberos authentication by ticket cache is not yet supported.</li>
</ul>
<p>The properties that apply to Hive connector security are listed in the
<a class="reference internal" href="#configuration-properties">Configuration Properties</a> table. Please see the
<a class="reference internal" href="hive-security.html"><span class="doc">Hive Security Configuration</span></a> section for a more detailed discussion of the
security options in the Hive connector.</p>
</div>
<div class="section" id="hdfs-permissions">
<h3>HDFS Permissions</h3>
<p>Before running any <code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> or <code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">...</span> <span class="pre">AS</span></code> statements
for Hive tables in Presto, you need to check that the operating system user
running the Presto server has access to the Hive warehouse directory on HDFS. The Hive warehouse
directory is specified by the configuration variable <code class="docutils literal"><span class="pre">hive.metastore.warehouse.dir</span></code>
in <code class="docutils literal"><span class="pre">hive-site.xml</span></code>, and the default value is <code class="docutils literal"><span class="pre">/user/hive/warehouse</span></code>. If that
is not the case, either add the following to <code class="docutils literal"><span class="pre">jvm.config</span></code> on all of the nodes:
<code class="docutils literal"><span class="pre">-DHADOOP_USER_NAME=USER</span></code>, where <code class="docutils literal"><span class="pre">USER</span></code> is an operating system user that has proper
permissions for the Hive warehouse directory, or start the Presto server as a user with
similar permissions. The <code class="docutils literal"><span class="pre">hive</span></code> user generally works as <code class="docutils literal"><span class="pre">USER</span></code>, since Hive is often
started with the <code class="docutils literal"><span class="pre">hive</span></code> user. If you run into HDFS permissions problems on
<code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">...</span> <span class="pre">AS</span></code>, remove <code class="docutils literal"><span class="pre">/tmp/presto-*</span></code> on HDFS, fix the user as described
above, then restart all of the Presto servers.</p>
</div>
</div>
<div class="section" id="configuration-properties">
<h2>Configuration Properties</h2>
<table border="1" class="docutils">
<colgroup>
<col width="41%" />
<col width="49%" />
<col width="10%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Description</th>
<th class="head">Default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.metastore.uri</span></code></td>
<td>The URI(s) of the Hive metastore to connect to using the
Thrift protocol. If multiple URIs are provided, the first
URI is used by default and the rest of the URIs are
fallback metastores. This property is required.
Example: <code class="docutils literal"><span class="pre">thrift://192.0.2.3:9083</span></code> or
<code class="docutils literal"><span class="pre">thrift://192.0.2.3:9083,thrift://192.0.2.4:9083</span></code></td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.config.resources</span></code></td>
<td>An optional comma-separated list of HDFS
configuration files. These files must exist on the
machines running Presto. Only specify this if
absolutely necessary to access HDFS.
Example: <code class="docutils literal"><span class="pre">/etc/hdfs-site.xml</span></code></td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.storage-format</span></code></td>
<td>The default file format used when creating new tables.</td>
<td><code class="docutils literal"><span class="pre">RCBINARY</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.compression-codec</span></code></td>
<td>The compression codec to use when writing files.</td>
<td><code class="docutils literal"><span class="pre">GZIP</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.force-local-scheduling</span></code></td>
<td>Force splits to be scheduled on the same node as the Hadoop
DataNode process serving the split data.  This is useful for
installations where Presto is collocated with every
DataNode.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.allow-drop-table</span></code></td>
<td>Allow the Hive connector to drop tables.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.allow-rename-table</span></code></td>
<td>Allow the Hive connector to rename tables.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.respect-table-format</span></code></td>
<td>Should new partitions be written using the existing table
format or the default Presto format?</td>
<td><code class="docutils literal"><span class="pre">true</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.immutable-partitions</span></code></td>
<td>Can new data be inserted into existing partitions?</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.max-partitions-per-writers</span></code></td>
<td>Maximum number of partitions per writer.</td>
<td>100</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.sse.enabled</span></code></td>
<td>Enable S3 server-side encryption.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.metastore.authentication.type</span></code></td>
<td>Hive metastore authentication type.
Possible values are <code class="docutils literal"><span class="pre">NONE</span></code> or <code class="docutils literal"><span class="pre">KERBEROS</span></code>.</td>
<td><code class="docutils literal"><span class="pre">NONE</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.metastore.service.principal</span></code></td>
<td>The Kerberos principal of the Hive metastore service.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.metastore.client.principal</span></code></td>
<td>The Kerberos principal that Presto will use when connecting
to the Hive metastore service.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.metastore.client.keytab</span></code></td>
<td>Hive metastore client keytab location.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.hdfs.authentication.type</span></code></td>
<td>HDFS authentication type.
Possible values are <code class="docutils literal"><span class="pre">NONE</span></code> or <code class="docutils literal"><span class="pre">KERBEROS</span></code>.</td>
<td><code class="docutils literal"><span class="pre">NONE</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.hdfs.impersonation.enabled</span></code></td>
<td>Enable HDFS end user impersonation.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.hdfs.presto.principal</span></code></td>
<td>The Kerberos principal that Presto will use when connecting
to HDFS.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.hdfs.presto.keytab</span></code></td>
<td>HDFS client keytab location.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.security</span></code></td>
<td>See <a class="reference internal" href="hive-security.html"><span class="doc">Hive Security Configuration</span></a>.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">security.config-file</span></code></td>
<td>Path of config file to use when <code class="docutils literal"><span class="pre">hive.security=file</span></code>.
See <a class="reference internal" href="hive-security.html#hive-file-based-authorization"><span class="std std-ref">File Based Authorization</span></a> for details.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.multi-file-bucketing.enabled</span></code></td>
<td>Enable support for multiple files per bucket for Hive
clustered tables. See <a class="reference internal" href="#clustered-tables"><span class="std std-ref">Clustered hive tables support</span></a></td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="amazon-s3-configuration">
<h2>Amazon S3 Configuration</h2>
<p>The Hive connector also allows querying data stored in Amazon S3.</p>
<p>To access tables stored in S3, you must specify the AWS credential properties
<code class="docutils literal"><span class="pre">hive.s3.aws-access-key</span></code> and <code class="docutils literal"><span class="pre">hive.s3.aws-secret-key</span></code>. Alternatively, you can use
<code class="docutils literal"><span class="pre">hive.s3.use-instance-credentials</span></code> which if set to true, enables retrieving temporary
<a class="reference external" href="http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/java-dg-roles.html">instance profile</a>
AWS credentials.</p>
<div class="section" id="sql-limitation-for-s3-tables">
<h3>SQL Limitation for S3 tables</h3>
<p>The SQL support for S3 tables is the same as for HDFS tables. Presto does not support creating external
tables in Hive (both HDFS and S3). If you want to create a table in Hive with data in S3, you have to do it from
<a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/HiveAws+HivingS3nRemotely">Hive</a>.</p>
<p>Also, <code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE..AS</span> <span class="pre">query</span></code>, where <code class="docutils literal"><span class="pre">query</span></code> is a <code class="docutils literal"><span class="pre">SELECT</span></code> query on the S3 table will not create the table
on S3. If you want to load data back to S3, you need to use <code class="docutils literal"><span class="pre">INSERT</span> <span class="pre">INTO</span></code> command.</p>
</div>
</div>
<div class="section" id="amazon-s3-configuration-properties">
<h2>Amazon S3 Configuration Properties</h2>
<table border="1" class="docutils">
<colgroup>
<col width="40%" />
<col width="48%" />
<col width="11%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Description</th>
<th class="head">Default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.aws-access-key</span></code></td>
<td>AWS Access key.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.aws-secret-key</span></code></td>
<td>AWS Secret key.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.use-instance-credentials</span></code></td>
<td>Instance profile credentials to use. This property is unused
if default credential properties are added.</td>
<td><code class="docutils literal"><span class="pre">true</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.connect-timeout</span></code></td>
<td>Amount of time that the HTTP connection will wait to
establish a connection before giving up.</td>
<td><code class="docutils literal"><span class="pre">5s</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.socket-timeout</span></code></td>
<td>Amount of time to wait for data to be transferred over an
established, open connection before the connection times
out and is closed.</td>
<td><code class="docutils literal"><span class="pre">5s</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.max-error-retries</span></code></td>
<td>Maximum retry count for retriable errors.</td>
<td><code class="docutils literal"><span class="pre">10</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.max-connections</span></code></td>
<td>See <a class="reference internal" href="#s3-max-connections"><span class="std std-ref">tuning section</span></a>.</td>
<td><code class="docutils literal"><span class="pre">500</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.ssl.enabled</span></code></td>
<td>Protocol to connect to AWS (HTTP or HTTPS).</td>
<td><code class="docutils literal"><span class="pre">true</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.pin-client-to-current-region</span></code></td>
<td>Use current AWS region.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.max-backoff-time</span></code></td>
<td>Maximum value of sleep time allowed during data read retry
mechanism. Uses exponential backoff pattern ranging from
1s to this value.</td>
<td><code class="docutils literal"><span class="pre">10</span> <span class="pre">minutes</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.max-retry-time</span></code></td>
<td>Retries read attempt till this threshold is reached or
<code class="docutils literal"><span class="pre">hive.s3.max-client-retries</span></code> value is crossed.</td>
<td><code class="docutils literal"><span class="pre">10</span> <span class="pre">minutes</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.max-client-retries</span></code></td>
<td>Reader fails if either <code class="docutils literal"><span class="pre">hive.s3.max-retry-time</span></code>
is reached or the number of attempts hits this value.</td>
<td><code class="docutils literal"><span class="pre">3</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.multipart.min-file-size</span></code></td>
<td>See <a class="reference internal" href="#s3-multipart-min-file"><span class="std std-ref">tuning section</span></a>.</td>
<td><code class="docutils literal"><span class="pre">16MB</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.multipart.min-part-size</span></code></td>
<td>See <a class="reference internal" href="#s3-multipart-min-part"><span class="std std-ref">tuning section</span></a>.</td>
<td><code class="docutils literal"><span class="pre">5MB</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.sse.enabled</span></code></td>
<td>Enable S3 server side encryption.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.staging-directory</span></code></td>
<td>Temporary directory for staging files before uploading
to S3.</td>
<td><code class="docutils literal"><span class="pre">/tmp</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="querying-hive-tables">
<h2>Querying Hive Tables</h2>
<p>The following table is an example Hive table from the <a class="reference external" href="https://cwiki.apache.org/confluence/display/Hive/Tutorial#Tutorial-UsageandExamples">Hive Tutorial</a>.
It can be created in Hive (not in Presto) using the following
Hive <code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> command:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>hive&gt; CREATE TABLE page_view (
    &gt;   viewTime INT,
    &gt;   userid BIGINT,
    &gt;   page_url STRING,
    &gt;   referrer_url STRING,
    &gt;   ip STRING COMMENT &#39;IP Address of the User&#39;)
    &gt; COMMENT &#39;This is the page view table&#39;
    &gt; PARTITIONED BY (dt STRING, country STRING)
    &gt; STORED AS SEQUENCEFILE;
OK
Time taken: 3.644 seconds
</pre></div>
</div>
<p>Assuming that this table was created in the <code class="docutils literal"><span class="pre">web</span></code> schema in
Hive, this table can be described in Presto:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">DESCRIBE</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span><span class="p">.</span><span class="n">page_view</span><span class="p">;</span>
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre><span></span>    Column    |  Type   | Null | Partition Key |        Comment
--------------+---------+------+---------------+------------------------
 viewtime     | bigint  | true | false         |
 userid       | bigint  | true | false         |
 page_url     | varchar | true | false         |
 referrer_url | varchar | true | false         |
 ip           | varchar | true | false         | IP Address of the User
 dt           | varchar | true | true          |
 country      | varchar | true | true          |
(7 rows)
</pre></div>
</div>
<p>This table can then be queried in Presto:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span><span class="p">.</span><span class="n">page_view</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="clustered-hive-tables-support">
<span id="clustered-tables"></span><h2>Clustered hive tables support</h2>
<p>By default presto supports only one data file per bucket per partition for clustered tables (Hive tables declared with <code class="docutils literal"><span class="pre">CLUSTERED</span> <span class="pre">BY</span></code> clause).
If number of files does not match number of buckets exception would be thrown.</p>
<p>To enable support for cases where there are more than one file per bucket, when multiple INSERTs are done to a single partition of the clustered table, you can use:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">hive.multi-file-bucketing.enabled</span></code> config property</li>
<li><code class="docutils literal"><span class="pre">multi_file_bucketing_enabled</span></code> session property (using <code class="docutils literal"><span class="pre">SET</span> <span class="pre">SESSION</span> <span class="pre">&lt;connector_name&gt;.multi_file_bucketing_enabled</span></code>)</li>
</ul>
</div></blockquote>
<p>Config property changes behaviour globally and session property can be used on per query basis.
The default value of session property is taken from config property.</p>
<p>If support for multiple files per bucket is enabled Presto will group the files in partition directory.
It will sort filenames lexicographically. Then it will treat part of filename up to first underscore character as bucket key.
This pattern matches naming convention of files in directory when Hive is used to inject data into table.</p>
<p>Presto will still validate if number of file groups matches number of buckets declared for table and fail if it does not.</p>
</div>
<div class="section" id="tuning">
<span id="tuning-pref-hive"></span><h2>Tuning</h2>
<p>The following configuration properties may have an impact on connector performance:</p>
<div class="section" id="hive-assume-canonical-partition-keys">
<h3><code class="docutils literal"><span class="pre">hive.assume-canonical-partition-keys</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong> Enable optimized metastore partition fetching for non-string partition keys. Setting this property allows to filter non-string partition keys while reading them from hive, based on the assumption that they are stored in canonical (java) format. This is disabled by default as hive allows to use non-canonical format as well (eg. boolean value <code class="docutils literal"><span class="pre">false</span></code> may be represented as <code class="docutils literal"><span class="pre">0</span></code>, <code class="docutils literal"><span class="pre">false</span></code>, <code class="docutils literal"><span class="pre">False</span></code> and more). Used correctly this property may drastically improve read time by reducing number of partition loaded from hive. Setting this property for non-canonical data format may cause erratic behavior.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-domain-compaction-threshold">
<h3><code class="docutils literal"><span class="pre">hive.domain-compaction-threshold</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">100</span></code></li>
<li><strong>Description:</strong> Maximum number of ranges/values allowed while reading hive data without compacting it. A higher value will cause more data fragmentation but allow the use of the row skipping feature when reading ORC data. Increasing this value may have a large impact on <code class="docutils literal"><span class="pre">IN</span></code> and <code class="docutils literal"><span class="pre">OR</span></code> clause performance in scenarios making use of row skipping.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-force-local-scheduling">
<span id="force-local-scheduling"></span><h3><code class="docutils literal"><span class="pre">hive.force-local-scheduling</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong> Force splits to be scheduled on the same node (ignoring normal node selection procedures) as the Hadoop DataNode process serving the split data. This is useful for installations where Presto is collocated with every DataNode and may increase queries time significantly. The drawback may be that if some data are accessed more often, the utilization of some nodes may be low even if the whole system is heavy loaded. See also <a class="reference internal" href="../admin/tuning.html"><span class="doc">node-scheduler.network-topology</span></a> if less strict constrain is preferred - especially if some nodes are overloaded and other are not fully utilized.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-max-initial-split-size">
<h3><code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.max-split-size</span></code> / <code class="docutils literal"><span class="pre">2</span></code> (<code class="docutils literal"><span class="pre">32</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong> This property describes the maximum size of the first <code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code> splits created for a query. the logic behind initial splits is described in <code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code>. Lower values will increase concurrency for small queries. This property represents the maximum size, as the real size may be lower when the amount of data to read is less than <code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code> (e.g. at the end of a block on a DataNode).</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-max-initial-splits">
<h3><code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">200</span></code></li>
<li><strong>Description:</strong> This property describes how many splits may be initially created for a single query using <code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code> instead of <code class="docutils literal"><span class="pre">hive.max-split-size</span></code>. A higher value will force more splits to have a smaller size (<code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code> is expected to be smaller than <code class="docutils literal"><span class="pre">hive.max-split-size</span></code>), effectively increasing the definition of what is considered a &#8220;small query&#8221;. The purpose of the smaller split size for the initial splits is to increase concurrency for smaller queries.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-max-outstanding-splits">
<h3><code class="docutils literal"><span class="pre">hive.max-outstanding-splits</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">1000</span></code></li>
<li><strong>Description:</strong> Limit on the nubmer of splits waiting to be served by a split source. After reaching this limit, writers will stop writing new splits until some of hteme are used by workers. Higher values will increase memory usage, but allow IO to be concentrated at one time, which may be faster and increase resource utilization.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-max-partitions-per-writers">
<h3><code class="docutils literal"><span class="pre">hive.max-partitions-per-writers</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">100</span></code></li>
<li><strong>Description:</strong> Maximum number of partitions per writer. A query will fail if it requires more partitions per writer than allowed by this property. It can be helpful to have queries beyond the expected maximum partitions to fail to help with error detection. Also it may allow to preactivly avoid out of memory problem.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-max-split-iterator-threads">
<h3><code class="docutils literal"><span class="pre">hive.max-split-iterator-threads</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">1000</span></code></li>
<li><strong>Description:</strong> This property describes how many threads may be used to iterate through splits when loading them to the worker nodes. A higher value may increase parallelism, but increased concurrency may cause too much time to be spent on context switching.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-max-split-size">
<h3><code class="docutils literal"><span class="pre">hive.max-split-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">64</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong> The maximum size of splits created after the initial splits. The logic for initial splits is described in <code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code>. A higher value will reduce parallelism. This may be desirable for very large queries and a stable cluster because it allows for more efficient processing of local data without the context switching, synchronization and data collection that result from parallelization. The optimal value should be aligned with the average query size in the system.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-metastore-partition-batch-size-max">
<h3><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">100</span></code></li>
<li><strong>Description:</strong> This together with <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code> defines the range of partition sizes read from Hive. The first partition is always of size <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code> and each following partition is two times bigger than previous up to <code class="docutils literal"><span class="pre">hive.mestastore.partition-batch-size.max</span></code> (the formula for partition size <code class="docutils literal"><span class="pre">n</span></code> is min(<code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code>, (<code class="docutils literal"><span class="pre">2``^``n</span></code>) * <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code>)). This algorithm allows for live adjustment of partition size according to the processing requirements. If the queries in the system will differ significantly from each other in size, then this range should be extended to better adjust to processing requirements. If the queries in the system will mostly be of the same size, then setting both values to the same maximally tuned value may give a slight edge in processing time.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-metastore-partition-batch-size-min">
<h3><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">10</span></code></li>
<li><strong>Description:</strong> See <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code>.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-orc-max-buffer-size">
<h3><code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong> Serves as default value for <code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code> session properties defining max size of ORC read operators. Higher value will allow bigger chunks to be processed but will decrease concurrency level.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-orc-max-merge-distance">
<h3><code class="docutils literal"><span class="pre">hive.orc.max-merge-distance</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">1</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong> Serves as the default value for the <code class="docutils literal"><span class="pre">orc_max_merge_distance</span></code> session property. Two reads from an ORC file may be merged into a single read if the distance between the requested data ranges in the data source is less than or equal to this value.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-orc-stream-buffer-size">
<h3><code class="docutils literal"><span class="pre">hive.orc.stream-buffer-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong> Serves as the default value for the <code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code> session property. It defines the maximum size of ORC read operators. A higher value will allow bigger chunks to be processed, but will decrease concurrency.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-orc-use-column-names">
<h3><code class="docutils literal"><span class="pre">hive.orc.use-column-names</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong> Access ORC columns using names from the file. By default, Hive access columns in ORC files using the order recoded in the Hive metastore. Setting this property allows to use columns names recorded in the ORC file instead.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-parquet-optimized-reader-enabled">
<span id="parquet-optimized-reader"></span><h3><code class="docutils literal"><span class="pre">hive.parquet-optimized-reader.enabled</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong> <em>Deprecated</em> Serves as default value for <code class="docutils literal"><span class="pre">parquet_optimized_reader_enabled</span></code> session property. Enables number of reader improvements introduced by alternative parquet implementation. The new reader supports vectorized reads, lazy loading, and predicate push down, all of which make the reader more efficient and typically reduces wall clock time for a query. However as the code has changed significantly it may or may not introduce some minor issues, so it can be disabled if some  problems with environment are noticed. This property enables/disables all optimizations except predicate push down as it is managed by <code class="docutils literal"><span class="pre">hive.parquet-predicate-pushdown.enabled</span></code> property.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-parquet-predicate-pushdown-enabled">
<h3><code class="docutils literal"><span class="pre">hive.parquet-predicate-pushdown.enabled</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong> <em>Deprecated</em> Serves as default value for <code class="docutils literal"><span class="pre">parquet_predicate_pushdown_enabled</span></code> sesssion property. See <a class="reference internal" href="#parquet-optimized-reader"><span class="std std-ref">hive.parquet-optimized-reader.enabled</span></a>.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-parquet-use-column-names">
<h3><code class="docutils literal"><span class="pre">hive.parquet.use-column-names</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong> Access Parquet columns using names from the file. By default, columns in Parquet files are accessed by their ordinal position in the Hive metastore. Setting this property allows access by column name recorded in the Parquet file instead.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-s3-max-connections">
<span id="s3-max-connections"></span><h3><code class="docutils literal"><span class="pre">hive.s3.max-connections</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">500</span></code></li>
<li><strong>Description:</strong> The maximum number of connections to S3 that may be open at a time by the S3 driver. A higher value may increase network utilization when a cluster is used on a high speed network. However, a higher values relies more on S3 servers being well configured for high parallelism.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-s3-multipart-min-file-size">
<span id="s3-multipart-min-file"></span><h3><code class="docutils literal"><span class="pre">hive.s3.multipart.min-file-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size, at least <code class="docutils literal"><span class="pre">16</span> <span class="pre">MB</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">16</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong> This property describes how big a file must be to be uploaded to an S3 cluster using the multipart upload feature. Amazon recommends using <code class="docutils literal"><span class="pre">100</span> <span class="pre">MB</span></code>, but a lower value may increase upload parallelism and decrease the <code class="docutils literal"><span class="pre">data</span> <span class="pre">lost</span></code>/<code class="docutils literal"><span class="pre">data</span> <span class="pre">sent</span></code> ratio in unstable network conditions.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-s3-multipart-min-part-size">
<span id="s3-multipart-min-part"></span><h3><code class="docutils literal"><span class="pre">hive.s3.multipart.min-part-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size, at least <code class="docutils literal"><span class="pre">5</span> <span class="pre">MB</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">5</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong> Defines the minimum part size for upload parts. Decreasing the minimum part size causes multipart uploads to be split into a larger number of smaller parts. Setting this value too low has a negative effect on transfer speeds, causing extra latency and network communication for each part.</li>
</ul>
</div></blockquote>
<p>There are also following session properties allowing to control connector behavior on single query basis:</p>
</div>
<div class="section" id="orc-max-buffer-size">
<h3><code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code> (<code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong> See <a class="reference internal" href="#tuning-pref-hive"><span class="std std-ref">hive.orc.max-buffer-size</span></a>.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="orc-max-merge-distance">
<h3><code class="docutils literal"><span class="pre">orc_max_merge_distance</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.orc.max-merge-distance</span></code> (<code class="docutils literal"><span class="pre">1</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong> See <a class="reference internal" href="#tuning-pref-hive"><span class="std std-ref">hive.orc.max-merge-distance</span></a>.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="orc-stream-buffer-size">
<h3><code class="docutils literal"><span class="pre">orc_stream_buffer_size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code> (<code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong> See <a class="reference internal" href="#tuning-pref-hive"><span class="std std-ref">hive.orc.max-buffer-size</span></a>.</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="hive-connector-limitations">
<h2>Hive Connector Limitations</h2>
<p><a class="reference internal" href="../sql/delete.html"><span class="doc">DELETE</span></a> is only supported if the <code class="docutils literal"><span class="pre">WHERE</span></code> clause matches entire partitions.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="hive-security.html" class="btn btn-neutral float-right" title="12.3. Hive Security Configuration" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="teradata.html" class="btn btn-neutral" title="12.1. Teradata Connector" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.152.1-t.0.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>