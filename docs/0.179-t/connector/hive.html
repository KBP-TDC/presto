

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>12.3. Hive Connector &mdash; Teradata Distribution of Presto 0.179-t.0.1 Documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/teradata.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="Teradata Distribution of Presto 0.179-t.0.1 Documentation" href="../index.html"/>
        <link rel="up" title="12. Teradata Supported Connectors" href="../connector.html"/>
        <link rel="next" title="12.4. Hive Security Configuration" href="hive-security.html"/>
        <link rel="prev" title="12.2. Cassandra Connector" href="cassandra.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Teradata Distribution of Presto
          

          
          </a>

          
            
            
              <div class="version">
                0.179-t.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">1. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system-requirements.html">2. System Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">3. Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sandbox-vms.html">4. Presto Installation on a Sandbox VM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../server-installation-cluster-manual.html">5. Presto Server Installation on a Cluster (Presto Admin and RPMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../server-installation-aws-emr-manual.html">6. Presto Server Installation on an AWS EMR (Presto Admin and RPMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../client.html">7. Presto Client Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../presto-admin/user-guide.html">8. Presto Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security.html">9. Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin.html">10. Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../querygrid.html">11. Teradata QueryGrid</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../connector.html">12. Teradata Supported Connectors</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="blackhole.html">12.1. Black Hole Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="cassandra.html">12.2. Cassandra Connector</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">12.3. Hive Connector</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#supported-file-types">Supported File Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuration">Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multiple-hive-clusters">Multiple Hive Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hdfs-configuration">HDFS Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hdfs-username">HDFS Username</a></li>
<li class="toctree-l4"><a class="reference internal" href="#accessing-hadoop-clusters-protected-with-kerberos-authentication">Accessing Hadoop clusters protected with Kerberos authentication</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hdfs-permissions">HDFS Permissions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#hive-configuration-properties">Hive Configuration Properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="#amazon-s3-configuration">Amazon S3 Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#s3-configuration-properties">S3 Configuration Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="#s3-credentials">S3 Credentials</a></li>
<li class="toctree-l4"><a class="reference internal" href="#custom-s3-credentials-provider">Custom S3 Credentials Provider</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tuning-properties">Tuning Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="#s3-data-encryption">S3 Data Encryption</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#schema-evolution">Schema Evolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tuning">Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hive-assume-canonical-partition-keys"><code class="docutils literal"><span class="pre">hive.assume-canonical-partition-keys</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-domain-compaction-threshold"><code class="docutils literal"><span class="pre">hive.domain-compaction-threshold</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-force-local-scheduling"><code class="docutils literal"><span class="pre">hive.force-local-scheduling</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-initial-split-size"><code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-initial-splits"><code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-outstanding-splits"><code class="docutils literal"><span class="pre">hive.max-outstanding-splits</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-partitions-per-writers"><code class="docutils literal"><span class="pre">hive.max-partitions-per-writers</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-split-iterator-threads"><code class="docutils literal"><span class="pre">hive.max-split-iterator-threads</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-max-split-size"><code class="docutils literal"><span class="pre">hive.max-split-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-metastore-partition-batch-size-max"><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-metastore-partition-batch-size-min"><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-max-buffer-size"><code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-max-merge-distance"><code class="docutils literal"><span class="pre">hive.orc.max-merge-distance</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-stream-buffer-size"><code class="docutils literal"><span class="pre">hive.orc.stream-buffer-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-orc-use-column-names"><code class="docutils literal"><span class="pre">hive.orc.use-column-names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-parquet-optimized-reader-enabled"><code class="docutils literal"><span class="pre">hive.parquet-optimized-reader.enabled</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-parquet-predicate-pushdown-enabled"><code class="docutils literal"><span class="pre">hive.parquet-predicate-pushdown.enabled</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-parquet-use-column-names"><code class="docutils literal"><span class="pre">hive.parquet.use-column-names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-s3-max-connections"><code class="docutils literal"><span class="pre">hive.s3.max-connections</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-s3-multipart-min-file-size"><code class="docutils literal"><span class="pre">hive.s3.multipart.min-file-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-s3-multipart-min-part-size"><code class="docutils literal"><span class="pre">hive.s3.multipart.min-part-size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#orc-max-buffer-size"><code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#orc-max-merge-distance"><code class="docutils literal"><span class="pre">orc_max_merge_distance</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#orc-stream-buffer-size"><code class="docutils literal"><span class="pre">orc_stream_buffer_size</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#clustered-hive-tables-support">Clustered Hive tables support</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hive-connector-limitations">Hive Connector Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hive-security.html">12.4. Hive Security Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="memory.html">12.5. Memory Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="jmx.html">12.6. JMX Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="mysql.html">12.7. MySQL Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="postgresql.html">12.8. PostgreSQL Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="sqlserver.html">12.9. SQL Server Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="system.html">12.10. System Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="teradata.html">12.11. Teradata Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="tpch.html">12.12. TPCH Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="tpcds.html">12.13. TPCDS Connector</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../community_connector.html">13. Community Supported Connectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">14. Community Supported Functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functions.html">15. Functions and Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../language.html">16. SQL Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sql.html">17. SQL Statement Syntax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">18. Migration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimizer.html">19. Query Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spill.html">20. Spilling to disk</a></li>
<li class="toctree-l1"><a class="reference internal" href="../develop.html">21. Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versions.html">22. Older Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release.html">23. Release Notes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Teradata Distribution of Presto</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
        <li><a href="../connector.html">12. Teradata Supported Connectors</a> &raquo;</li>
      
    <li>12.3. Hive Connector</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="../_sources/connector/hive.txt" rel="nofollow"> View page source</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hive-connector">
<h1>12.3. Hive Connector</h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#overview" id="id1">Overview</a></li>
<li><a class="reference internal" href="#supported-file-types" id="id2">Supported File Types</a></li>
<li><a class="reference internal" href="#configuration" id="id3">Configuration</a></li>
<li><a class="reference internal" href="#hive-configuration-properties" id="id4">Hive Configuration Properties</a></li>
<li><a class="reference internal" href="#amazon-s3-configuration" id="id5">Amazon S3 Configuration</a></li>
<li><a class="reference internal" href="#schema-evolution" id="id6">Schema Evolution</a></li>
<li><a class="reference internal" href="#examples" id="id7">Examples</a></li>
<li><a class="reference internal" href="#tuning" id="id8">Tuning</a></li>
<li><a class="reference internal" href="#clustered-hive-tables-support" id="id9">Clustered Hive tables support</a></li>
<li><a class="reference internal" href="#hive-connector-limitations" id="id10">Hive Connector Limitations</a></li>
</ul>
</div>
<div class="section" id="overview">
<h2>Overview</h2>
<p>The Hive connector allows querying data stored in a Hive
data warehouse. Hive is a combination of three components:</p>
<ul class="simple">
<li>Data files in varying formats that are typically stored in the
Hadoop Distributed File System (HDFS) or in Amazon S3.</li>
<li>Metadata about how the data files are mapped to schemas and tables.
This metadata is stored in a database such as MySQL and is accessed
via the Hive metastore service.</li>
<li>A query language called HiveQL. This query language is executed
on a distributed computing framework such as MapReduce or Tez.</li>
</ul>
<p>Presto only uses the first two components: the data and the metadata.
It does not use HiveQL or any part of Hive&#8217;s execution environment.</p>
</div>
<div class="section" id="supported-file-types">
<h2>Supported File Types</h2>
<p>The following file types are supported for the Hive connector:</p>
<ul class="simple">
<li>ORC</li>
<li>Parquet</li>
<li>Avro</li>
<li>RCFile</li>
<li>SequenceFile</li>
<li>JSON</li>
<li>Text</li>
</ul>
</div>
<div class="section" id="configuration">
<h2>Configuration</h2>
<p>The Hive connector supports Apache Hadoop 2.x and derivative distributions
including Cloudera CDH 5 and Hortonworks Data Platform (HDP).</p>
<p>Create <code class="docutils literal"><span class="pre">~/.prestoadmin/catalog/hive.properties</span></code> with the following
contents to mount the <code class="docutils literal"><span class="pre">hive-hadoop2</span></code> connector as the <code class="docutils literal"><span class="pre">hive</span></code>
catalog, replacing <code class="docutils literal"><span class="pre">example.net:9083</span></code> with the correct host and port
for your Hive metastore Thrift service:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>connector.name=hive-hadoop2
hive.metastore.uri=thrift://example.net:9083
</pre></div>
</div>
<p>Use <code class="docutils literal"><span class="pre">presto-admin</span></code> to deploy the connector file. See <a class="reference internal" href="../presto-admin/installation/presto-catalog-installation.html"><span class="doc">Adding a Catalog</span></a>.</p>
<div class="section" id="multiple-hive-clusters">
<h3>Multiple Hive Clusters</h3>
<p>You can have as many catalogs as you need, so if you have additional
Hive clusters, simply add another properties file to <code class="docutils literal"><span class="pre">~/.prestoadmin/catalog</span></code>
with a different name (making sure it ends in <code class="docutils literal"><span class="pre">.properties</span></code>). For
example, if you name the property file <code class="docutils literal"><span class="pre">sales.properties</span></code>, Presto
will create a catalog named <code class="docutils literal"><span class="pre">sales</span></code> using the configured connector.</p>
</div>
<div class="section" id="hdfs-configuration">
<h3>HDFS Configuration</h3>
<p>For basic setups, Presto configures the HDFS client automatically and
does not require any configuration files. In some cases, such as when using
federated HDFS or NameNode high availability, it is necessary to specify
additional HDFS client options in order to access your HDFS cluster. To do so,
add the <code class="docutils literal"><span class="pre">hive.config.resources</span></code> property to reference your HDFS config files:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
</pre></div>
</div>
<p>Only specify additional configuration files if necessary for your setup.
We also recommend reducing the configuration files to have the minimum
set of required properties, as additional properties may cause problems.</p>
<p>The configuration files must exist on all Presto nodes. If you are
referencing existing Hadoop config files, make sure to copy them to
any Presto nodes that are not running Hadoop.</p>
</div>
<div class="section" id="hdfs-username">
<h3>HDFS Username</h3>
<p>When not using Kerberos with HDFS, Presto will access HDFS using the
OS user of the Presto process. For example, if Presto is running as
<code class="docutils literal"><span class="pre">nobody</span></code>, it will access HDFS as <code class="docutils literal"><span class="pre">nobody</span></code>. You can override this
username by setting the <code class="docutils literal"><span class="pre">HADOOP_USER_NAME</span></code> system property in the
Presto <a class="reference internal" href="../installation/deployment.html#presto-jvm-config"><span class="std std-ref">JVM Config</span></a>, replacing <code class="docutils literal"><span class="pre">hdfs_user</span></code> with the
appropriate username:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>-DHADOOP_USER_NAME=hdfs_user
</pre></div>
</div>
</div>
<div class="section" id="accessing-hadoop-clusters-protected-with-kerberos-authentication">
<h3>Accessing Hadoop clusters protected with Kerberos authentication</h3>
<p>Kerberos authentication is supported for both HDFS and the Hive metastore.
However, Kerberos authentication by ticket cache is not yet supported.</p>
<p>The properties that apply to Hive connector security are listed in the
<a class="reference internal" href="#hive-configuration-properties">Hive Configuration Properties</a> table. Please see the
<a class="reference internal" href="hive-security.html"><span class="doc">Hive Security Configuration</span></a> section for a more detailed discussion of the
security options in the Hive connector.</p>
</div>
<div class="section" id="hdfs-permissions">
<h3>HDFS Permissions</h3>
<p>Before running any <code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> or <code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">...</span> <span class="pre">AS</span></code> statements
for Hive tables in Presto, you need to check that the operating system user
running the Presto server has access to the Hive warehouse directory on HDFS. The Hive warehouse
directory is specified by the configuration variable <code class="docutils literal"><span class="pre">hive.metastore.warehouse.dir</span></code>
in <code class="docutils literal"><span class="pre">hive-site.xml</span></code>, and the default value is <code class="docutils literal"><span class="pre">/user/hive/warehouse</span></code>. If that
is not the case, either add the following to <code class="docutils literal"><span class="pre">jvm.config</span></code> on all of the nodes:
<code class="docutils literal"><span class="pre">-DHADOOP_USER_NAME=USER</span></code>, where <code class="docutils literal"><span class="pre">USER</span></code> is an operating system user that has proper
permissions for the Hive warehouse directory, or start the Presto server as a user with
similar permissions. The <code class="docutils literal"><span class="pre">hive</span></code> user generally works as <code class="docutils literal"><span class="pre">USER</span></code>, since Hive is often
started with the <code class="docutils literal"><span class="pre">hive</span></code> user. If you run into HDFS permissions problems on
<code class="docutils literal"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">...</span> <span class="pre">AS</span></code>, remove <code class="docutils literal"><span class="pre">/tmp/presto-*</span></code> on HDFS, fix the user as described
above, then restart all of the Presto servers.</p>
</div>
</div>
<div class="section" id="hive-configuration-properties">
<h2>Hive Configuration Properties</h2>
<table border="1" class="docutils">
<colgroup>
<col width="41%" />
<col width="49%" />
<col width="10%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Description</th>
<th class="head">Default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.metastore.uri</span></code></td>
<td>The URI(s) of the Hive metastore to connect to using the
Thrift protocol. If multiple URIs are provided, the first
URI is used by default and the rest of the URIs are
fallback metastores. This property is required.
Example: <code class="docutils literal"><span class="pre">thrift://192.0.2.3:9083</span></code> or
<code class="docutils literal"><span class="pre">thrift://192.0.2.3:9083,thrift://192.0.2.4:9083</span></code></td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.config.resources</span></code></td>
<td>An optional comma-separated list of HDFS
configuration files. These files must exist on the
machines running Presto. Only specify this if
absolutely necessary to access HDFS.
Example: <code class="docutils literal"><span class="pre">/etc/hdfs-site.xml</span></code></td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.storage-format</span></code></td>
<td>The default file format used when creating new tables.</td>
<td><code class="docutils literal"><span class="pre">RCBINARY</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.compression-codec</span></code></td>
<td>The compression codec to use when writing files.</td>
<td><code class="docutils literal"><span class="pre">GZIP</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.force-local-scheduling</span></code></td>
<td>See <a class="reference internal" href="#force-local-scheduling"><span class="std std-ref">tuning section</span></a></td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.respect-table-format</span></code></td>
<td>Should new partitions be written using the existing table
format or the default Presto format?</td>
<td><code class="docutils literal"><span class="pre">true</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.immutable-partitions</span></code></td>
<td>Can new data be inserted into existing partitions?</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.max-partitions-per-writers</span></code></td>
<td>Maximum number of partitions per writer.</td>
<td>100</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.metastore.authentication.type</span></code></td>
<td>Hive metastore authentication type.
Possible values are <code class="docutils literal"><span class="pre">NONE</span></code> or <code class="docutils literal"><span class="pre">KERBEROS</span></code>.</td>
<td><code class="docutils literal"><span class="pre">NONE</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.metastore.service.principal</span></code></td>
<td>The Kerberos principal of the Hive metastore service.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.metastore.client.principal</span></code></td>
<td>The Kerberos principal that Presto will use when connecting
to the Hive metastore service.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.metastore.client.keytab</span></code></td>
<td>Hive metastore client keytab location.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.hdfs.authentication.type</span></code></td>
<td>HDFS authentication type.
Possible values are <code class="docutils literal"><span class="pre">NONE</span></code> or <code class="docutils literal"><span class="pre">KERBEROS</span></code>.</td>
<td><code class="docutils literal"><span class="pre">NONE</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.hdfs.impersonation.enabled</span></code></td>
<td>Enable HDFS end user impersonation.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.hdfs.presto.principal</span></code></td>
<td>The Kerberos principal that Presto will use when connecting
to HDFS.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.hdfs.presto.keytab</span></code></td>
<td>HDFS client keytab location.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.security</span></code></td>
<td>See <a class="reference internal" href="hive-security.html"><span class="doc">Hive Security Configuration</span></a>.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">security.config-file</span></code></td>
<td>Path of config file to use when <code class="docutils literal"><span class="pre">hive.security=file</span></code>.
See <a class="reference internal" href="hive-security.html#hive-file-based-authorization"><span class="std std-ref">File Based Authorization</span></a> for details.</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.non-managed-table-writes-enabled</span></code></td>
<td>Enable writes to non-managed (external) Hive tables.</td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.create-non-managed-table-enabled</span></code></td>
<td>Enable creating non-managed (external) Hive tables.</td>
<td><code class="docutils literal"><span class="pre">true</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.multi-file-bucketing.enabled</span></code></td>
<td>Enable support for multiple files per bucket for Hive
clustered tables. See <a class="reference internal" href="#clustered-tables"><span class="std std-ref">Clustered Hive tables support</span></a></td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.empty-bucketed-partitions.enabled</span></code></td>
<td>Enable support for clustered tables with empty partitions.
See <a class="reference internal" href="#clustered-tables"><span class="std std-ref">Clustered Hive tables support</span></a></td>
<td><code class="docutils literal"><span class="pre">false</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="amazon-s3-configuration">
<h2>Amazon S3 Configuration</h2>
<p>The Hive Connector can read and write tables that are stored in S3.
This is accomplished by having a table or database location that
uses an S3 prefix rather than an HDFS prefix.</p>
<p>Presto uses its own S3 filesystem for the URI prefixes
<code class="docutils literal"><span class="pre">s3://</span></code>, <code class="docutils literal"><span class="pre">s3n://</span></code> and  <code class="docutils literal"><span class="pre">s3a://</span></code>.</p>
<div class="section" id="s3-configuration-properties">
<h3>S3 Configuration Properties</h3>
<table border="1" class="docutils">
<colgroup>
<col width="40%" />
<col width="60%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.use-instance-credentials</span></code></td>
<td>Use the EC2 metadata service to retrieve API credentials
(defaults to <code class="docutils literal"><span class="pre">true</span></code>). This works with IAM roles in EC2.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.aws-access-key</span></code></td>
<td>Default AWS access key to use.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.aws-secret-key</span></code></td>
<td>Default AWS secret key to use.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.endpoint</span></code></td>
<td>The S3 storage endpoint server. This can be used to
connect to an S3-compatible storage system instead
of AWS. When using v4 signatures, it is recommended to
set this to the AWS region-specific endpoint
(e.g., <code class="docutils literal"><span class="pre">http[s]://&lt;bucket&gt;.s3-&lt;AWS-region&gt;.amazonaws.com</span></code>).</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.signer-type</span></code></td>
<td>Specify a different signer type for S3-compatible storage.
Example: <code class="docutils literal"><span class="pre">S3SignerType</span></code> for v2 signer type</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.staging-directory</span></code></td>
<td>Local staging directory for data written to S3.
This defaults to the Java temporary directory specified
by the JVM system property <code class="docutils literal"><span class="pre">java.io.tmpdir</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.pin-client-to-current-region</span></code></td>
<td>Pin S3 requests to the same region as the EC2
instance where Presto is running (defaults to <code class="docutils literal"><span class="pre">false</span></code>).</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.ssl.enabled</span></code></td>
<td>Use HTTPS to communicate with the S3 API (defaults to <code class="docutils literal"><span class="pre">true</span></code>).</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.sse.enabled</span></code></td>
<td>Use S3 server-side encryption (defaults to <code class="docutils literal"><span class="pre">false</span></code>).</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.sse.type</span></code></td>
<td>The type of key management for S3 server-side encryption.
Use <code class="docutils literal"><span class="pre">S3</span></code> for S3 managed or <code class="docutils literal"><span class="pre">KMS</span></code> for KMS-managed keys
(defaults to <code class="docutils literal"><span class="pre">S3</span></code>).</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.sse.kms-key-id</span></code></td>
<td>The KMS Key ID to use for S3 server-side encryption with
KMS-managed keys. If not set, the default key is used.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.kms-key-id</span></code></td>
<td>If set, use S3 client-side encryption and use the AWS
KMS to store encryption keys and use the value of
this property as the KMS Key ID for newly created
objects.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.encryption-materials-provider</span></code></td>
<td>If set, use S3 client-side encryption and use the
value of this property as the fully qualified name of
a Java class which implements the AWS SDK&#8217;s
<code class="docutils literal"><span class="pre">EncryptionMaterialsProvider</span></code> interface.   If the
class also implements <code class="docutils literal"><span class="pre">Configurable</span></code> from the Hadoop
API, the Hadoop configuration will be passed in after
the object has been created.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="s3-credentials">
<h3>S3 Credentials</h3>
<p>If you are running Presto on Amazon EC2 using EMR or another facility,
it is highly recommended that you set <code class="docutils literal"><span class="pre">hive.s3.use-instance-credentials</span></code>
to <code class="docutils literal"><span class="pre">true</span></code> and use IAM Roles for EC2 to govern access to S3. If this is
the case, your EC2 instances will need to be assigned an IAM Role which
grants appropriate access to the data stored in the S3 bucket(s) you wish
to use.  This is much cleaner than setting AWS access and secret keys in
the <code class="docutils literal"><span class="pre">hive.s3.aws-access-key</span></code> and <code class="docutils literal"><span class="pre">hive.s3.aws-secret-key</span></code> settings, and also
allows EC2 to automatically rotate credentials on a regular basis without
any additional work on your part.</p>
</div>
<div class="section" id="custom-s3-credentials-provider">
<h3>Custom S3 Credentials Provider</h3>
<p>You can configure a custom S3 credentials provider by setting the Hadoop
configuration property <code class="docutils literal"><span class="pre">presto.s3.credentials-provider</span></code> to be the
fully qualified class name of a custom AWS credentials provider
implementation. This class must implement the
<a class="reference external" href="http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/AWSCredentialsProvider.html">AWSCredentialsProvider</a>
interface and provide a two-argument constructor that takes a
<code class="docutils literal"><span class="pre">java.net.URI</span></code> and a Hadoop <code class="docutils literal"><span class="pre">org.apache.hadoop.conf.Configuration</span></code>
as arguments. A custom credentials provider can be used to provide
temporary credentials from STS (using <code class="docutils literal"><span class="pre">STSSessionCredentialsProvider</span></code>),
IAM role-based credentials (using <code class="docutils literal"><span class="pre">STSAssumeRoleSessionCredentialsProvider</span></code>),
or credentials for a specific use case (e.g., bucket/user specific credentials).
This Hadoop configuration property must be set in the Hadoop configuration
files referenced by the <code class="docutils literal"><span class="pre">hive.config.resources</span></code> Hive connector property.</p>
</div>
<div class="section" id="tuning-properties">
<h3>Tuning Properties</h3>
<p>The following tuning properties affect the behavior of the client
used by the Presto S3 filesystem when communicating with S3.
Most of these parameters affect settings on the <code class="docutils literal"><span class="pre">ClientConfiguration</span></code>
object associated with the <code class="docutils literal"><span class="pre">AmazonS3Client</span></code>.</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="53%" />
<col width="14%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Description</th>
<th class="head">Default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.max-error-retries</span></code></td>
<td>Maximum number of error retries, set on the S3 client.</td>
<td><code class="docutils literal"><span class="pre">10</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.max-client-retries</span></code></td>
<td>Maximum number of read attempts to retry.</td>
<td><code class="docutils literal"><span class="pre">5</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.max-backoff-time</span></code></td>
<td>Use exponential backoff starting at 1 second up to
this maximum value when communicating with S3.</td>
<td><code class="docutils literal"><span class="pre">10</span> <span class="pre">minutes</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.max-retry-time</span></code></td>
<td>Maximum time to retry communicating with S3.</td>
<td><code class="docutils literal"><span class="pre">10</span> <span class="pre">minutes</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.connect-timeout</span></code></td>
<td>TCP connect timeout.</td>
<td><code class="docutils literal"><span class="pre">5</span> <span class="pre">seconds</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.socket-timeout</span></code></td>
<td>TCP socket read timeout.</td>
<td><code class="docutils literal"><span class="pre">5</span> <span class="pre">seconds</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.max-connections</span></code></td>
<td>Maximum number of simultaneous open connections to S3.</td>
<td><code class="docutils literal"><span class="pre">500</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">hive.s3.multipart.min-file-size</span></code></td>
<td>Minimum file size before multi-part upload to S3 is used.</td>
<td><code class="docutils literal"><span class="pre">16</span> <span class="pre">MB</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hive.s3.multipart.min-part-size</span></code></td>
<td>Minimum multi-part upload part size.</td>
<td><code class="docutils literal"><span class="pre">5</span> <span class="pre">MB</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="s3-data-encryption">
<h3>S3 Data Encryption</h3>
<p>Presto supports reading and writing encrypted data in S3 using both
server-side encryption with S3 managed keys and client-side encryption using
either the Amazon KMS or a software plugin to manage AES encryption keys.</p>
<p>With <a class="reference external" href="http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html">S3 server-side encryption</a>,
(called <em>SSE-S3</em> in the Amazon documentation) the S3 infrastructure takes care of all encryption and decryption
work (with the exception of SSL to the client, assuming you have <code class="docutils literal"><span class="pre">hive.s3.ssl.enabled</span></code> set to <code class="docutils literal"><span class="pre">true</span></code>).
S3 also manages all the encryption keys for you. To enable this, set <code class="docutils literal"><span class="pre">hive.s3.sse.enabled</span></code> to <code class="docutils literal"><span class="pre">true</span></code>.</p>
<p>With <a class="reference external" href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html">S3 client-side encryption</a>,
S3 stores encrypted data and the encryption keys are managed outside of the S3 infrastructure. Data is encrypted
and decrypted by Presto instead of in the S3 infrastructure. In this case, encryption keys can be managed
either by using the AWS KMS or your own key management system. To use the AWS KMS for key management, set
<code class="docutils literal"><span class="pre">hive.s3.kms-key-id</span></code> to the UUID of a KMS key. Your AWS credentials or EC2 IAM role will need to be
granted permission to use the given key as well.</p>
<p>To use a custom encryption key management system, set <code class="docutils literal"><span class="pre">hive.s3.encryption-materials-provider</span></code> to the
fully qualified name of a class which implements the
<a class="reference external" href="http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/EncryptionMaterialsProvider.html">EncryptionMaterialsProvider</a>
interface from the AWS Java SDK. This class will have to be accessible to the Hive Connector through the
classpath and must be able to communicate with your custom key management system. If this class also implements
the <code class="docutils literal"><span class="pre">org.apache.hadoop.conf.Configurable</span></code> interface from the Hadoop Java API, then the Hadoop configuration
will be passed in after the object instance is created and before it is asked to provision or retrieve any
encryption keys.</p>
</div>
</div>
<div class="section" id="schema-evolution">
<h2>Schema Evolution</h2>
<p>Hive allows the partitions in a table to have a different schema than the
table. This occurs when the column types of a table are changed after
partitions already exist (that use the original column types). The Hive
connector supports this by allowing the same conversions as Hive:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">varchar</span></code> to and from <code class="docutils literal"><span class="pre">tinyint</span></code>, <code class="docutils literal"><span class="pre">smallint</span></code>, <code class="docutils literal"><span class="pre">integer</span></code> and <code class="docutils literal"><span class="pre">bigint</span></code></li>
<li><code class="docutils literal"><span class="pre">real</span></code> to <code class="docutils literal"><span class="pre">double</span></code></li>
<li>Widening conversions for integers, such as <code class="docutils literal"><span class="pre">tinyint</span></code> to <code class="docutils literal"><span class="pre">smallint</span></code></li>
</ul>
<p>Any conversion failure will result in null, which is the same behavior
as Hive. For example, converting the string <code class="docutils literal"><span class="pre">'foo'</span></code> to a number,
or converting the string <code class="docutils literal"><span class="pre">'1234'</span></code> to a <code class="docutils literal"><span class="pre">tinyint</span></code> (which has a
maximum value of <code class="docutils literal"><span class="pre">127</span></code>).</p>
</div>
<div class="section" id="examples">
<h2>Examples</h2>
<p>The Hive connector supports querying and manipulating Hive tables and schemas
(databases). While some uncommon operations will need to be performed using
Hive directly, most operations can be performed using Presto.</p>
<p>Create a new Hive schema named <code class="docutils literal"><span class="pre">web</span></code> that will store tables in an
S3 bucket named <code class="docutils literal"><span class="pre">my-bucket</span></code>:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">SCHEMA</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span>
<span class="k">WITH</span> <span class="p">(</span><span class="k">location</span> <span class="o">=</span> <span class="s1">&#39;s3://my-bucket/&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Create a new Hive table named <code class="docutils literal"><span class="pre">page_views</span></code> in the <code class="docutils literal"><span class="pre">web</span></code> schema
that is stored using the ORC file format, partitioned by date and
country, and bucketed by user into <code class="docutils literal"><span class="pre">50</span></code> buckets (note that Hive
requires the partition columns to be the last columns in the table):</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span><span class="p">.</span><span class="n">page_views</span> <span class="p">(</span>
  <span class="n">view_time</span> <span class="k">timestamp</span><span class="p">,</span>
  <span class="n">user_id</span> <span class="nb">bigint</span><span class="p">,</span>
  <span class="n">page_url</span> <span class="nb">varchar</span><span class="p">,</span>
  <span class="n">ds</span> <span class="nb">date</span><span class="p">,</span>
  <span class="n">country</span> <span class="nb">varchar</span>
<span class="p">)</span>
<span class="k">WITH</span> <span class="p">(</span>
  <span class="n">format</span> <span class="o">=</span> <span class="s1">&#39;ORC&#39;</span><span class="p">,</span>
  <span class="n">partitioned_by</span> <span class="o">=</span> <span class="nb">ARRAY</span><span class="p">[</span><span class="s1">&#39;ds&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">],</span>
  <span class="n">bucketed_by</span> <span class="o">=</span> <span class="nb">ARRAY</span><span class="p">[</span><span class="s1">&#39;user_id&#39;</span><span class="p">],</span>
  <span class="n">bucket_count</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Drop a partition from the <code class="docutils literal"><span class="pre">page_views</span></code> table:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">DELETE</span> <span class="k">FROM</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span><span class="p">.</span><span class="n">page_views</span>
<span class="k">WHERE</span> <span class="n">ds</span> <span class="o">=</span> <span class="nb">DATE</span> <span class="s1">&#39;2016-08-09&#39;</span>
  <span class="k">AND</span> <span class="n">country</span> <span class="o">=</span> <span class="s1">&#39;US&#39;</span>
</pre></div>
</div>
<p>Query the <code class="docutils literal"><span class="pre">page_views</span></code> table:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span><span class="p">.</span><span class="n">page_views</span>
</pre></div>
</div>
<p>Create an external Hive table named <code class="docutils literal"><span class="pre">request_logs</span></code> that points at
existing data in S3:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span><span class="p">.</span><span class="n">request_logs</span> <span class="p">(</span>
  <span class="n">request_time</span> <span class="k">timestamp</span><span class="p">,</span>
  <span class="n">url</span> <span class="nb">varchar</span><span class="p">,</span>
  <span class="n">ip</span> <span class="nb">varchar</span><span class="p">,</span>
  <span class="n">user_agent</span> <span class="nb">varchar</span>
<span class="p">)</span>
<span class="k">WITH</span> <span class="p">(</span>
  <span class="n">format</span> <span class="o">=</span> <span class="s1">&#39;TEXTFILE&#39;</span><span class="p">,</span>
  <span class="n">external_location</span> <span class="o">=</span> <span class="s1">&#39;s3://my-bucket/data/logs/&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Drop the external table <code class="docutils literal"><span class="pre">request_logs</span></code>. This only drops the metadata
for the table. The referenced data directory is not deleted:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span><span class="p">.</span><span class="n">request_logs</span>
</pre></div>
</div>
<p>Drop a schema:</p>
<div class="highlight-sql"><div class="highlight"><pre><span></span><span class="k">DROP</span> <span class="k">SCHEMA</span> <span class="n">hive</span><span class="p">.</span><span class="n">web</span>
</pre></div>
</div>
</div>
<div class="section" id="tuning">
<span id="tuning-pref-hive"></span><h2>Tuning</h2>
<p>The following configuration properties may have an impact on connector performance:</p>
<div class="section" id="hive-assume-canonical-partition-keys">
<h3><code class="docutils literal"><span class="pre">hive.assume-canonical-partition-keys</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Enable optimized metastore partition fetching for non-string partition keys. Setting this
property allows to filter non-string partition keys while reading them from hive, based on
the assumption that they are stored in canonical (java) format. This is disabled by default
as hive allows to use non-canonical format as well (eg. boolean value <code class="docutils literal"><span class="pre">false</span></code> may be
represented as <code class="docutils literal"><span class="pre">0</span></code>, <code class="docutils literal"><span class="pre">false</span></code>, <code class="docutils literal"><span class="pre">False</span></code> and more). Used correctly this property may
drastically improve read time by reducing number of partition loaded from hive. Setting
this property for non-canonical data format may cause erratic behavior.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-domain-compaction-threshold">
<h3><code class="docutils literal"><span class="pre">hive.domain-compaction-threshold</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">100</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Maximum number of ranges/values allowed while reading hive data without compacting it.
A higher value will cause more data fragmentation but allow the use of the row skipping
feature when reading ORC data. Increasing this value may have a large impact on <code class="docutils literal"><span class="pre">IN</span></code>
and <code class="docutils literal"><span class="pre">OR</span></code> clause performance in scenarios making use of row skipping.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-force-local-scheduling">
<span id="force-local-scheduling"></span><h3><code class="docutils literal"><span class="pre">hive.force-local-scheduling</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Force splits to be scheduled on the same node (ignoring normal node selection procedures)
as the Hadoop DataNode process serving the split data. This is useful for installations
where Presto is collocated with every DataNode and may decrease queries time significantly.
The drawback may be that if some data are accessed more often, the utilization of some nodes
may be low even if the whole system is heavy loaded.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-max-initial-split-size">
<h3><code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.max-split-size</span></code> / <code class="docutils literal"><span class="pre">2</span></code> (<code class="docutils literal"><span class="pre">32</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>This property describes the maximum size of the first <code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code>
splits created for a query. the logic behind initial splits is described in
<code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code>. Lower values will increase concurrency for small queries.
This property represents the maximum size, as the real size may be lower when the amount
of data to read is less than <code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code> (e.g. at the end of a
block on a DataNode).</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-max-initial-splits">
<h3><code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">200</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>This property describes how many splits may be initially created for a single query
using <code class="docutils literal"><span class="pre">hive.max-initial-split-size</span></code> instead of <code class="docutils literal"><span class="pre">hive.max-split-size</span></code>. A higher
value will force more splits to have a smaller size (<code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code> is
expected to be smaller than <code class="docutils literal"><span class="pre">hive.max-split-size</span></code>), effectively increasing the
definition of what is considered a &#8220;small query&#8221;. The purpose of the smaller split
size for the initial splits is to increase concurrency for smaller queries.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-max-outstanding-splits">
<h3><code class="docutils literal"><span class="pre">hive.max-outstanding-splits</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">1000</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Limit on the nubmer of splits waiting to be served by a split source. After reaching
this limit, writers will stop writing new splits until some of hteme are used by workers.
Higher values will increase memory usage, but allow IO to be concentrated at one time,
which may be faster and increase resource utilization.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-max-partitions-per-writers">
<h3><code class="docutils literal"><span class="pre">hive.max-partitions-per-writers</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">100</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Maximum number of partitions per writer. A query will fail if it requires more partitions
per writer than allowed by this property. It can be helpful to have queries beyond the
expected maximum partitions to fail to help with error detection. Also it may allow to
preactivly avoid out of memory problem.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-max-split-iterator-threads">
<h3><code class="docutils literal"><span class="pre">hive.max-split-iterator-threads</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">1000</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>This property describes how many threads may be used to iterate through splits when loading
them to the worker nodes. A higher value may increase parallelism, but increased concurrency
may cause too much time to be spent on context switching.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-max-split-size">
<h3><code class="docutils literal"><span class="pre">hive.max-split-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">64</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>The maximum size of splits created after the initial splits. The logic for initial splits is
described in <code class="docutils literal"><span class="pre">hive.max-initial-splits</span></code>. A higher value will reduce parallelism. This may be
desirable for very large queries and a stable cluster because it allows for more efficient
processing of local data without the context switching, synchronization and data collection
that result from parallelization. The optimal value should be aligned with the average query
size in the system.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-metastore-partition-batch-size-max">
<h3><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">100</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>This together with <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code> defines the range of partition
sizes read from Hive. The first partition is always of size <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code>
and each following partition is two times bigger than previous up to
<code class="docutils literal"><span class="pre">hive.mestastore.partition-batch-size.max</span></code> (the formula for partition size <code class="docutils literal"><span class="pre">n</span></code> is
min(<code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code>, (<code class="docutils literal"><span class="pre">2``^``n</span></code>) * <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code>)).
This algorithm allows for live adjustment of partition size according to the processing requirements.
If the queries in the system will differ significantly from each other in size, then this range should be
extended to better adjust to processing requirements. If the queries in the system will mostly be of the
same size, then setting both values to the same maximally tuned value may give a slight edge in
processing time.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-metastore-partition-batch-size-min">
<h3><code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.min</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">10</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>See <code class="docutils literal"><span class="pre">hive.metastore.partition-batch-size.max</span></code>.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-orc-max-buffer-size">
<h3><code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Serves as default value for <code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code> session properties defining max size
of ORC read operators. Higher value will allow bigger chunks to be processed but will
decrease concurrency level.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-orc-max-merge-distance">
<h3><code class="docutils literal"><span class="pre">hive.orc.max-merge-distance</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">1</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Serves as the default value for the <code class="docutils literal"><span class="pre">orc_max_merge_distance</span></code> session property. Two reads
from an ORC file may be merged into a single read if the distance between the requested data
ranges in the data source is less than or equal to this value.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-orc-stream-buffer-size">
<h3><code class="docutils literal"><span class="pre">hive.orc.stream-buffer-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Serves as the default value for the <code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code> session property. It defines the
maximum size of ORC read operators. A higher value will allow bigger chunks to be processed,
but will decrease concurrency.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-orc-use-column-names">
<h3><code class="docutils literal"><span class="pre">hive.orc.use-column-names</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Access ORC columns using names from the file. By default, Hive access columns in ORC files
using the order recoded in the Hive metastore. Setting this property allows to use columns
names recorded in the ORC file instead.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-parquet-optimized-reader-enabled">
<span id="parquet-optimized-reader"></span><h3><code class="docutils literal"><span class="pre">hive.parquet-optimized-reader.enabled</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div><em>Deprecated</em> Serves as default value for <code class="docutils literal"><span class="pre">parquet_optimized_reader_enabled</span></code> session property.
Enables number of reader improvements introduced by alternative parquet implementation.
The new reader supports vectorized reads, lazy loading, and predicate push down, all of which
make the reader more efficient and typically reduces wall clock time for a query. However as
the code has changed significantly it may or may not introduce some minor issues, so it can be
disabled if some  problems with environment are noticed. This property enables/disables all
optimizations except predicate push down as it is managed by
<code class="docutils literal"><span class="pre">hive.parquet-predicate-pushdown.enabled</span></code> property.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-parquet-predicate-pushdown-enabled">
<h3><code class="docutils literal"><span class="pre">hive.parquet-predicate-pushdown.enabled</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div><em>Deprecated</em> Serves as default value for <code class="docutils literal"><span class="pre">parquet_predicate_pushdown_enabled</span></code> sesssion property.
See <a class="reference internal" href="#parquet-optimized-reader"><span class="std std-ref">hive.parquet-optimized-reader.enabled</span></a>.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-parquet-use-column-names">
<h3><code class="docutils literal"><span class="pre">hive.parquet.use-column-names</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Boolean</span></code></li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">false</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Access Parquet columns using names from the file. By default, columns in Parquet files are accessed by
their ordinal position in the Hive metastore. Setting this property allows access by column name recorded
in the Parquet file instead.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-s3-max-connections">
<h3><code class="docutils literal"><span class="pre">hive.s3.max-connections</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">Integer</span></code> (at least <code class="docutils literal"><span class="pre">1</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">500</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>The maximum number of connections to S3 that may be open at a time by the S3 driver. A higher value
may increase network utilization when a cluster is used on a high speed network. However, a higher
values relies more on S3 servers being well configured for high parallelism.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-s3-multipart-min-file-size">
<h3><code class="docutils literal"><span class="pre">hive.s3.multipart.min-file-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size, at least <code class="docutils literal"><span class="pre">16</span> <span class="pre">MB</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">16</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>This property describes how big a file must be to be uploaded to an S3 cluster using the multipart
upload feature. Amazon recommends using <code class="docutils literal"><span class="pre">100</span> <span class="pre">MB</span></code>, but a lower value may increase upload parallelism
and decrease the <code class="docutils literal"><span class="pre">data</span> <span class="pre">lost</span></code>/<code class="docutils literal"><span class="pre">data</span> <span class="pre">sent</span></code> ratio in unstable network conditions.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="hive-s3-multipart-min-part-size">
<h3><code class="docutils literal"><span class="pre">hive.s3.multipart.min-part-size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size, at least <code class="docutils literal"><span class="pre">5</span> <span class="pre">MB</span></code>)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">5</span> <span class="pre">MB</span></code></li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>Defines the minimum part size for upload parts. Decreasing the minimum part size causes multipart
uploads to be split into a larger number of smaller parts. Setting this value too low has a negative
effect on transfer speeds, causing extra latency and network communication for each part.</div></blockquote>
</div></blockquote>
<p>There are also following session properties allowing to control connector behavior on single query basis:</p>
</div>
<div class="section" id="orc-max-buffer-size">
<h3><code class="docutils literal"><span class="pre">orc_max_buffer_size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code> (<code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>See <a class="reference internal" href="#tuning-pref-hive"><span class="std std-ref">hive.orc.max-buffer-size</span></a>.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="orc-max-merge-distance">
<h3><code class="docutils literal"><span class="pre">orc_max_merge_distance</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.orc.max-merge-distance</span></code> (<code class="docutils literal"><span class="pre">1</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>See <a class="reference internal" href="#tuning-pref-hive"><span class="std std-ref">hive.orc.max-merge-distance</span></a>.</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="orc-stream-buffer-size">
<h3><code class="docutils literal"><span class="pre">orc_stream_buffer_size</span></code></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Type:</strong> <code class="docutils literal"><span class="pre">String</span></code> (data size)</li>
<li><strong>Default value:</strong> <code class="docutils literal"><span class="pre">hive.orc.max-buffer-size</span></code> (<code class="docutils literal"><span class="pre">8</span> <span class="pre">MB</span></code>)</li>
<li><strong>Description:</strong></li>
</ul>
<blockquote>
<div>See <a class="reference internal" href="#tuning-pref-hive"><span class="std std-ref">hive.orc.max-buffer-size</span></a>.</div></blockquote>
</div></blockquote>
</div>
</div>
<div class="section" id="clustered-hive-tables-support">
<span id="clustered-tables"></span><h2>Clustered Hive tables support</h2>
<p>By default Presto supports only one data file per bucket per partition for clustered tables (Hive tables declared with <code class="docutils literal"><span class="pre">CLUSTERED</span> <span class="pre">BY</span></code> clause).
If number of files does not match number of buckets exception would be thrown.</p>
<p>To enable support for cases where there are more than one file per bucket, when multiple INSERTs were done to a single partition of the clustered table, you can use:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">hive.multi-file-bucketing.enabled</span></code> config property</li>
<li><code class="docutils literal"><span class="pre">multi_file_bucketing_enabled</span></code> session property (using <code class="docutils literal"><span class="pre">SET</span> <span class="pre">SESSION</span> <span class="pre">&lt;connector_name&gt;.multi_file_bucketing_enabled</span></code>)</li>
</ul>
</div></blockquote>
<p>Config property changes behaviour globally and session property can be used on per query basis.
The default value of session property is taken from config property.</p>
<p>If support for multiple files per bucket is enabled Presto will group the files in partition directory.
It will sort filenames lexicographically. Then it will treat part of filename up to first underscore character as bucket key.
This pattern matches naming convention of files in directory when Hive is used to inject data into table.</p>
<p>Presto will still validate if number of file groups matches number of buckets declared for table and fail if it does not.</p>
<p>Similarly by default empty partitions (partitions with no files) are not allowed for clustered Hive tables.
To enable support for empty paritions you can use:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">hive.empty-bucketed-partitions.enabled</span></code> config property</li>
<li><code class="docutils literal"><span class="pre">empty_bucketed_partitions_enabled</span></code> session property (using <code class="docutils literal"><span class="pre">SET</span> <span class="pre">SESSION</span> <span class="pre">&lt;connector_name&gt;.empty_bucketed_partitions_enabled</span></code>)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="hive-connector-limitations">
<h2>Hive Connector Limitations</h2>
<p><a class="reference internal" href="../sql/delete.html"><span class="doc">DELETE</span></a> is only supported if the <code class="docutils literal"><span class="pre">WHERE</span></code> clause matches entire partitions.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="hive-security.html" class="btn btn-neutral float-right" title="12.4. Hive Security Configuration" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cassandra.html" class="btn btn-neutral" title="12.2. Cassandra Connector" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.179-t.0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>